{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ecbc85-47c2-454d-90c8-71d415f83288",
   "metadata": {},
   "source": [
    "# Demo for launching an experiment set  \n",
    "\n",
    "*Objectif*: **Comparing some models variability**\n",
    "\n",
    "An experiment set is a collection of experiments that are part of the same evaluation scenario. \n",
    "In this notebook, we're comparing how the (maximum) number of chunks influences the model's performance.\n",
    "\n",
    "To conduct these experiments, one approach consists by creating an empty experiment set (via POST /experiment_set) and then add a list of experiments to it (via POST /experiment with the reference to the experimentset_id). Each experiment should have all parameters the same, except for the (maximum) number of chunks.\n",
    "\n",
    "Alternatively, the /experiment_set endpoint offers a convenient feature called cv (short for cross-validation). This feature includes two key parameters:\n",
    "\n",
    "- **common_params**: These are the parameters that will be shared across all experiments in the set.\n",
    "- **grid_params**: This allows you to specify a list of varying values for any parameter.\n",
    "\n",
    "\n",
    "Both **commons_params** and **grid_params** accept all the parameter defined by the ExperimentSetCreate schema.  \n",
    "The experiments will be generated by combining the **common_params** with each unique set of values from the cartesian product of the lists provided in **grid_params**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fb6038d3-def4-4fc4-8ae2-4327af3e919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import dotenv\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "sys.path.append(\"..\")\n",
    "from api.utils import log_and_raise_for_status\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "ALBERT_API_URL = \"https://albert.api.etalab.gouv.fr/v1\"\n",
    "ALBERT_API_KEY = os.getenv(\"ALBERT_API_KEY\")\n",
    "MFS_API_URL = \"https://franceservices.staging.etalab.gouv.fr/api/v1\"\n",
    "MFS_API_KEY = os.getenv(\"MFS_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ffe4f9-9dda-429d-a8ea-1b783abfc505",
   "metadata": {},
   "source": [
    "## Designing and running an experiment set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6156a8a-6812-49e7-9c22-3d8df99853b0",
   "metadata": {},
   "source": [
    "## Reading and showing results\n",
    "\n",
    "-> Show the mean and std score, for each metrics, across the experiment repetition. The std show the variability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b60dd8b-e7b5-4a08-86c7-1025c5b80f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created expset: fs_variability_v2 (9)\n"
     ]
    }
   ],
   "source": [
    "# Designing my experiments\n",
    "# --\n",
    "expset_name = \"fs_variability_v2\"\n",
    "expset_readme = \"omparing some models variability.\"\n",
    "metrics = [\"answer_relevancy\", \"judge_exactness\", \"judge_notator\", \"output_length\", \"generation_time\"]\n",
    "common_params = {\n",
    "    \"dataset\" : \"MFS_questions_v01\",\n",
    "    \"sampling_params\" : {\"temperature\": 0.2},\n",
    "    \"metrics\" : metrics\n",
    "}\n",
    "grid_params = {\n",
    "    \"model\": [\n",
    "        {\"name\": \"AgentPublic/llama3-instruct-8b\",         \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY},\n",
    "        {\"name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",  \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY},\n",
    "        {\"name\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\", \"base_url\": ALBERT_API_URL, \"api_key\": ALBERT_API_KEY},\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Lauching the experiment set\n",
    "expset = {\n",
    "    \"name\" : expset_name, \n",
    "    \"readme\": expset_readme,\n",
    "    \"cv\": {\"common_params\": common_params, \"grid_params\": grid_params, \"repeat\":10}\n",
    "}\n",
    "response = requests.post(f'{API_URL}/v1/experiment_set', json=expset)\n",
    "resp = response.json()\n",
    "if \"id\" in resp:\n",
    "    expset_id = resp[\"id\"]\n",
    "    print(f'Created expset: {resp[\"name\"]} ({resp[\"id\"]})')\n",
    "else:\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "beff216c-0ea4-48b5-a71b-dc980313aa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['judge_notator', 'output_length', 'generation_time', 'answer_relevancy', 'judge_exactness'])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m arr_metrics \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(arr_all[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_metrics\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr_metrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_metrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      9\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m,i)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:827\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    817\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    818\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    824\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[1;32m    825\u001b[0m         )\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 827\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/internals/construction.py:314\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    308\u001b[0m     _copy \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    309\u001b[0m         copy_on_sanitize\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, dtype))\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     )\n\u001b[1;32m    313\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(values, copy\u001b[38;5;241m=\u001b[39m_copy)\n\u001b[0;32m--> 314\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     values \u001b[38;5;241m=\u001b[39m _prep_ndarraylike(values, copy\u001b[38;5;241m=\u001b[39mcopy_on_sanitize)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/internals/construction.py:592\u001b[0m, in \u001b[0;36m_ensure_2d\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    590\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape((values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=()"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "arr_metrics = np.array(arr_all[0])\n",
    "print(df_metrics.keys())\n",
    "df = pd.DataFrame(arr_metrics.T, columns=df_metrics.keys())\n",
    "\n",
    "for i, column in enumerate(df.columns, 1):\n",
    "    plt.subplot(3,3,i)\n",
    "    sns.histplot(df[column], bins=20, stat=\"probability\", element=\"bars\", kde=True)\n",
    "    plt.ylabel(\"\") \n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa4e6b-0b8c-4112-b7fe-a21bb50d114c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "177816f4-1105-497d-a14b-7178850d36f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>judge_exactness</th>\n",
       "      <th>judge_notator</th>\n",
       "      <th>output_length</th>\n",
       "      <th>generation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AgentPublic/llama3-instruct-8b</th>\n",
       "      <td>0.92 ± 0.02</td>\n",
       "      <td>0.03 ± 0.02</td>\n",
       "      <td>3.37 ± 0.15</td>\n",
       "      <td>309.97 ± 7.37</td>\n",
       "      <td>6.69 ± 0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-llama/Meta-Llama-3.1-8B-Instruct</th>\n",
       "      <td>0.92 ± 0.03</td>\n",
       "      <td>0.02 ± 0.02</td>\n",
       "      <td>3.60 ± 0.12</td>\n",
       "      <td>296.09 ± 10.61</td>\n",
       "      <td>3.80 ± 0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-llama/Meta-Llama-3.1-70B-Instruct</th>\n",
       "      <td>0.96 ± 0.02</td>\n",
       "      <td>0.08 ± 0.03</td>\n",
       "      <td>4.87 ± 0.22</td>\n",
       "      <td>240.28 ± 13.35</td>\n",
       "      <td>9.10 ± 0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       answer_relevancy judge_exactness  \\\n",
       "AgentPublic/llama3-instruct-8b              0.92 ± 0.02     0.03 ± 0.02   \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct       0.92 ± 0.03     0.02 ± 0.02   \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct      0.96 ± 0.02     0.08 ± 0.03   \n",
       "\n",
       "                                       judge_notator   output_length  \\\n",
       "AgentPublic/llama3-instruct-8b           3.37 ± 0.15   309.97 ± 7.37   \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct    3.60 ± 0.12  296.09 ± 10.61   \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct   4.87 ± 0.22  240.28 ± 13.35   \n",
       "\n",
       "                                       generation_time  \n",
       "AgentPublic/llama3-instruct-8b             6.69 ± 0.13  \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct      3.80 ± 0.14  \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct     9.10 ± 0.60  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read results\n",
    "# --\n",
    "df_all = None # multi-dimensional DataFrame\n",
    "arr_all = [] # keep references of source array per metric metrics \n",
    "\n",
    "# Fetch results and compute macro metrics (mean, std etc).\n",
    "# --\n",
    "response = requests.get(f'{API_URL}/v1/experiment_set/{expset_id}')\n",
    "expset = response.json()\n",
    "rows = []\n",
    "for i, exp in enumerate(expset[\"experiments\"]):\n",
    "    # Get an experiment result\n",
    "    exp_id = exp[\"id\"]\n",
    "    response = requests.get(f'{API_URL}/v1/experiment/{exp_id}?with_results=true')\n",
    "    experiment = response.json()\n",
    "    # experiment[\"name\"] # Name of the experiment\n",
    "    if experiment[\"experiment_status\"] != \"finished\":\n",
    "        print(f\"Warning: experiment {exp_id} is not finished yet...\")\n",
    "    results = experiment[\"results\"]\n",
    "    model = experiment[\"model\"][\"name\"]\n",
    "    \n",
    "    #Add an observation row from the observation_table (mean, std etc)\n",
    "    row = {\"model\": model}\n",
    "    rows.append(row)\n",
    "    metric_arrs = {}\n",
    "    arr_all.append(metric_arrs)\n",
    "    for metric_results in results: \n",
    "        metric = metric_results[\"metric_name\"]\n",
    "        arr = np.array([x[\"score\"] for x in metric_results[\"observation_table\"] if pd.notna(x[\"score\"])])\n",
    "        row[(metric, 'mean')] = np.mean(arr)\n",
    "        row[(metric, 'std')] = np.std(arr)\n",
    "        row[(metric, 'median')] = np.median(arr)\n",
    "        row[(metric, 'mean_std')] = f\"{arr.mean():.2f} ± {arr.std():.2f}\"  # Formatting as 'mean±std'\n",
    "        row[(metric, 'support')] = len(arr)\n",
    "        metric_arrs[metric] = arr\n",
    "    \n",
    "df_all = pd.DataFrame(rows)\n",
    "df_all.set_index('model', inplace=True)\n",
    "df_all.columns = pd.MultiIndex.from_tuples(df_all.columns)\n",
    "final_df = df_all.xs('mean', axis=1, level=1)\n",
    "#final_df[\"support\"] = supports # for debugging\n",
    "\n",
    "# Group and average the result of the experiments by models\n",
    "# --\n",
    "def format_metrics(row):\n",
    "   metrics = {}\n",
    "   for metric in final_df.columns.levels[0]:\n",
    "       mean_value = row[(metric, 'mean')]\n",
    "       std_value = row[(metric, 'std')]\n",
    "       metrics[metric] = f\"{mean_value:.2f} ± {std_value:.2f}\"\n",
    "   return pd.Series(metrics)\n",
    "final_df = final_df.groupby(level=0).agg(['mean', 'std'])  # groupby \"model\"\n",
    "final_df.index.name = None\n",
    "final_df = final_df.apply(format_metrics, axis=1) # final formating\n",
    "final_df = final_df.reindex([m[\"name\"] for m in grid_params[\"model\"]]) # reorder rows\n",
    "final_df = final_df[metrics] # reorder columns\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
