{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ecbc85-47c2-454d-90c8-71d415f83288",
   "metadata": {},
   "source": [
    "# Demo for launching an experiment set  \n",
    "\n",
    "*Objectif*: **Comparing the impact of the `limit` parameters on a RAG model**\n",
    "\n",
    "An experiment set is a collection of experiments that are part of the same evaluation scenario. \n",
    "In this notebook, we're comparing how the (maximum) number of chunks influences the model's performance.\n",
    "\n",
    "To conduct these experiments, one approach consists by creating an empty experiment set (via POST /experimentset) and then add a list of experiments to it (via POST /experiment with the reference to the experimentset_id). Each experiment should have all parameters the same, except for the (maximum) number of chunks.\n",
    "\n",
    "Alternatively, the /experimentset endpoint offers a convenient feature called cv (short for cross-validation). This feature includes two key parameters:\n",
    "\n",
    "- **common_params**: These are the parameters that will be shared across all experiments in the set.\n",
    "- **grid_params**: This allows you to specify a list of varying values for any parameter.\n",
    "\n",
    "\n",
    "Both **commons_params** and **grid_params** accept all the parameter defined by the ExperimentSetCreate schema.  \n",
    "The experiments will be generated by combining the **common_params** with each unique set of values from the cartesian product of the lists provided in **grid_params**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fb6038d3-def4-4fc4-8ae2-4327af3e919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import dotenv\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "sys.path.append(\"..\")\n",
    "from api.utils import log_and_raise_for_status\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "ALBERT_API_URL = \"https://albert.api.etalab.gouv.fr/v1\"\n",
    "ALBERT_API_KEY = os.getenv(\"ALBERT_API_KEY\")\n",
    "MFS_API_URL = \"https://franceservices.staging.etalab.gouv.fr/api/v1\"\n",
    "MFS_API_KEY = os.getenv(\"MFS_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ffe4f9-9dda-429d-a8ea-1b783abfc505",
   "metadata": {},
   "source": [
    "## Designing an experiment set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1b60dd8b-e7b5-4a08-86c7-1025c5b80f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created expset: fs_rag_limit (5)\n"
     ]
    }
   ],
   "source": [
    "# Designing my experiments\n",
    "expset_name = \"fs_rag_limit\"\n",
    "expset_readme = \"Comparing the impact of the `limit` parameters on a RAG model.\"\n",
    "metrics = [\"answer_relevancy\", \"judge_exactness\", \"judge_notator\",  \"output_length\", \"generation_time\"]\n",
    "common_params = {\n",
    "    \"dataset\" : \"MFS_questions_v01_head\",\n",
    "    \"model\" : {\"name\": \"AgentPublic/llama3-instruct-8b\", \"base_url\": MFS_API_URL, \"api_key\": MFS_API_KEY},\n",
    "    \"sampling_params\" : {\"temperature\": 0.2},\n",
    "    \"metrics\" : metrics\n",
    "}\n",
    "grid_params = {\n",
    "    \"model\": [{\"extra_params\": {\"rag\": {\"mode\":\"rag\", \"limit\":i}}} for i in [1, 2, 3, 4, 5, 7, 10, 15, 20]],\n",
    "}\n",
    "\n",
    "# Lauching the experiment set\n",
    "expset = {\n",
    "    \"name\" : expset_name, \n",
    "    \"readme\": expset_readme,\n",
    "    \"cv\": {\"common_params\": common_params, \"grid_params\": grid_params}\n",
    "}\n",
    "response = requests.post(f'{API_URL}/v1/experimentset', json=expset)\n",
    "resp = response.json()\n",
    "if \"id\" in resp:\n",
    "    expset_id = resp[\"id\"]\n",
    "    print(f'Created expset: {resp[\"name\"]} ({resp[\"id\"]})')\n",
    "else:\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6156a8a-6812-49e7-9c22-3d8df99853b0",
   "metadata": {},
   "source": [
    "## Read results and showing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "177816f4-1105-497d-a14b-7178850d36f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>judge_completude</th>\n",
       "      <th>judge_exactness</th>\n",
       "      <th>judge_notator</th>\n",
       "      <th>output_length</th>\n",
       "      <th>generation_time</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AgentPublic/llama3-instruct-8b</th>\n",
       "      <td>0.90 ± 0.23</td>\n",
       "      <td>26.41 ± 22.27</td>\n",
       "      <td>0.03 ± 0.16</td>\n",
       "      <td>3.41 ± 1.97</td>\n",
       "      <td>308.33 ± 93.23</td>\n",
       "      <td>6.64 ± 1.99</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-llama/Meta-Llama-3.1-8B-Instruct</th>\n",
       "      <td>0.95 ± 0.15</td>\n",
       "      <td>30.00 ± 20.16</td>\n",
       "      <td>0.05 ± 0.22</td>\n",
       "      <td>3.97 ± 2.27</td>\n",
       "      <td>296.13 ± 113.57</td>\n",
       "      <td>3.82 ± 1.43</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-llama/Meta-Llama-3.1-70B-Instruct</th>\n",
       "      <td>0.92 ± 0.22</td>\n",
       "      <td>35.00 ± 24.73</td>\n",
       "      <td>0.05 ± 0.22</td>\n",
       "      <td>4.85 ± 2.63</td>\n",
       "      <td>261.89 ± 109.02</td>\n",
       "      <td>9.42 ± 4.01</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfs-rag-baseline</th>\n",
       "      <td>0.90 ± 0.18</td>\n",
       "      <td>42.05 ± 26.91</td>\n",
       "      <td>0.21 ± 0.40</td>\n",
       "      <td>5.36 ± 2.59</td>\n",
       "      <td>119.41 ± 66.23</td>\n",
       "      <td>4.31 ± 1.80</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       answer_relevancy judge_completude  \\\n",
       "AgentPublic/llama3-instruct-8b              0.90 ± 0.23    26.41 ± 22.27   \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct       0.95 ± 0.15    30.00 ± 20.16   \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct      0.92 ± 0.22    35.00 ± 24.73   \n",
       "mfs-rag-baseline                            0.90 ± 0.18    42.05 ± 26.91   \n",
       "\n",
       "                                       judge_exactness judge_notator  \\\n",
       "AgentPublic/llama3-instruct-8b             0.03 ± 0.16   3.41 ± 1.97   \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct      0.05 ± 0.22   3.97 ± 2.27   \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct     0.05 ± 0.22   4.85 ± 2.63   \n",
       "mfs-rag-baseline                           0.21 ± 0.40   5.36 ± 2.59   \n",
       "\n",
       "                                          output_length generation_time  \\\n",
       "AgentPublic/llama3-instruct-8b           308.33 ± 93.23     6.64 ± 1.99   \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct   296.13 ± 113.57     3.82 ± 1.43   \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct  261.89 ± 109.02     9.42 ± 4.01   \n",
       "mfs-rag-baseline                         119.41 ± 66.23     4.31 ± 1.80   \n",
       "\n",
       "                                           toxicity         bias  \n",
       "AgentPublic/llama3-instruct-8b          0.00 ± 0.00  0.00 ± 0.00  \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct   0.00 ± 0.00  0.00 ± 0.00  \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct  0.00 ± 0.00  0.00 ± 0.00  \n",
       "mfs-rag-baseline                        0.00 ± 0.00  0.00 ± 0.00  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read results\n",
    "# --\n",
    "df_all = [] # list of results per experiment/model\n",
    "supports = [] # Store the support for debugging: if some answer/observation failed, the support can be less than the dataset size\n",
    "\n",
    "response = requests.get(f'{API_URL}/v1/experimentset/{expset_id}')\n",
    "expset = response.json()\n",
    "\n",
    "for exp in expset[\"experiments\"]:\n",
    "    # Get an experiment result\n",
    "    exp_id = exp[\"id\"]\n",
    "    response = requests.get(f'{API_URL}/v1/experiment/{exp_id}?with_results=true')\n",
    "    experiment = response.json()\n",
    "    # experiment[\"name\"] # Name of the experiment\n",
    "    if experiment[\"experiment_status\"] != \"finished\":\n",
    "        print(f\"Warning: experiment {exp_id} is not finished yet...\")\n",
    "    results = experiment[\"results\"]\n",
    "    \n",
    "    # Build a result dataframe  from the observation_table (mean, std etc)\n",
    "    df_metrics = {}\n",
    "    metric_support = []\n",
    "    supports.append(metric_support)\n",
    "    for metric_results in results: \n",
    "        metric_name = metric_results[\"metric_name\"]\n",
    "        arr = np.array([x[\"score\"] for x in metric_results[\"observation_table\"] if pd.notna(x[\"score\"])])\n",
    "        df = pd.DataFrame([[\n",
    "                np.mean(arr), # mean\n",
    "                np.std(arr), # std\n",
    "                np.median(arr), # median\n",
    "                f\"{arr.mean():.2f} ± {arr.std():.2f}\",  # Formatting as 'mean±std'\n",
    "                len(arr), # support\n",
    "            ]], columns=[\"mean\", \"std\", \"median\", \"mean_std\", \"support\"])\n",
    "        df_metrics[metric_name] = df\n",
    "        metric_support.append(len(arr))\n",
    "    \n",
    "    # Stack the mean_std final measure\n",
    "    limit = exp[\"model\"][\"extra_params\"][\"limit\"]\n",
    "    name = f\"mfs-rag-baseline_limit{limit}\" \n",
    "    df = pd.DataFrame({metric_name:df[\"mean_std\"].iloc[0] for metric_name, df in sorted(df_metrics.items())}, index=[name])\n",
    "    df_all.append(df)\n",
    "\n",
    "final_df = pd.concat(df_all)\n",
    "#final_df[\"support\"] = supports # for debugging\n",
    "# Reorder columns\n",
    "final_df = final_df[metrics]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688dcf6-2e78-48dd-85db-197bd4b767a6",
   "metadata": {},
   "source": [
    "## What is inside an experiment result ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6c3b601d-8340-49cc-aec5-fff803fb946c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1276,\n",
       " 'created_at': '2024-11-09T12:57:32.560445',\n",
       " 'score': 1.0,\n",
       " 'observation': 'The score is 1.00 because the response perfectly addresses the question without any irrelevant information. Great job!',\n",
       " 'num_line': 21,\n",
       " 'error_msg': None,\n",
       " 'execution_time': 4}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what's inside an experiment result\n",
    "# --\n",
    "metric_index = 2\n",
    "len(results) # number of metrics\n",
    "list(results[metric_index]) # the keys of the dict representing the result object\n",
    "len(results[metric_index][\"observation_table\"]) # number of observation -> one per dataset line.\n",
    "results[metric_index][\"observation_table\"][0] # the actual \"observation\" for one metric, for one line in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
