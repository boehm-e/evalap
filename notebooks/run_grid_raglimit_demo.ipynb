{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ecbc85-47c2-454d-90c8-71d415f83288",
   "metadata": {},
   "source": [
    "# Demo for launching an experiment set  \n",
    "\n",
    "*Objectif*: **Comparing the impact of the `limit` parameters on a RAG model**\n",
    "\n",
    "An experiment set is a collection of experiments that are part of the same evaluation scenario. \n",
    "In this notebook, we're comparing how the (maximum) number of chunks influences the model's performance.\n",
    "\n",
    "To conduct these experiments, one approach consists by creating an empty experiment set (via POST /experiment_set) and then add a list of experiments to it (via POST /experiment with the reference to the experimentset_id). Each experiment should have all parameters the same, except for the (maximum) number of chunks.\n",
    "\n",
    "Alternatively, the /experiment_set endpoint offers a convenient feature called cv (short for cross-validation). This feature includes two key parameters:\n",
    "\n",
    "- **common_params**: These are the parameters that will be shared across all experiments in the set.\n",
    "- **grid_params**: This allows you to specify a list of varying values for any parameter.\n",
    "\n",
    "\n",
    "Both **commons_params** and **grid_params** accept all the parameter defined by the ExperimentSetCreate schema.  \n",
    "The experiments will be generated by combining the **common_params** with each unique set of values from the cartesian product of the lists provided in **grid_params**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fb6038d3-def4-4fc4-8ae2-4327af3e919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import dotenv\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "dotenv.load_dotenv(\"../.env\")\n",
    "sys.path.append(\"..\")\n",
    "from api.utils import log_and_raise_for_status\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "ALBERT_API_URL = \"https://albert.api.etalab.gouv.fr/v1\"\n",
    "ALBERT_API_KEY = os.getenv(\"ALBERT_API_KEY\")\n",
    "MFS_API_URL = \"https://franceservices.staging.etalab.gouv.fr/api/v1\"\n",
    "MFS_API_KEY = os.getenv(\"MFS_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ffe4f9-9dda-429d-a8ea-1b783abfc505",
   "metadata": {},
   "source": [
    "## Designing and running an experiment set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1b60dd8b-e7b5-4a08-86c7-1025c5b80f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created expset: fs_rag_limit (5)\n"
     ]
    }
   ],
   "source": [
    "# Designing my experiments\n",
    "# --\n",
    "expset_name = \"fs_rag_limit\"\n",
    "expset_readme = \"Comparing the impact of the `limit` parameters on a RAG model.\"\n",
    "metrics = [\"answer_relevancy\", \"judge_exactness\", \"judge_notator\", \"output_length\", \"generation_time\"]\n",
    "common_params = {\n",
    "    \"dataset\" : \"MFS_questions_v01\",\n",
    "    \"model\" : {\"name\": \"AgentPublic/llama3-instruct-8b\", \"base_url\": MFS_API_URL, \"api_key\": MFS_API_KEY},\n",
    "    \"sampling_params\" : {\"temperature\": 0.2},\n",
    "    \"metrics\" : metrics\n",
    "}\n",
    "grid_params = {\n",
    "    \"model\": [{\"extra_params\": {\"rag\": {\"mode\":\"rag\", \"limit\":i}}} for i in [1, 2, 3, 4, 5, 7, 10, 15, 20]],\n",
    "}\n",
    "\n",
    "# Lauching the experiment set\n",
    "expset = {\n",
    "    \"name\" : expset_name, \n",
    "    \"readme\": expset_readme,\n",
    "    \"cv\": {\"common_params\": common_params, \"grid_params\": grid_params}\n",
    "}\n",
    "response = requests.post(f'{API_URL}/v1/experiment_set', json=expset)\n",
    "resp = response.json()\n",
    "if \"id\" in resp:\n",
    "    expset_id = resp[\"id\"]\n",
    "    print(f'Created expset: {resp[\"name\"]} ({resp[\"id\"]})')\n",
    "else:\n",
    "    print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6156a8a-6812-49e7-9c22-3d8df99853b0",
   "metadata": {},
   "source": [
    "## Reading and showing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "177816f4-1105-497d-a14b-7178850d36f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>judge_completude</th>\n",
       "      <th>judge_exactness</th>\n",
       "      <th>judge_notator</th>\n",
       "      <th>output_length</th>\n",
       "      <th>generation_time</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AgentPublic/llama3-instruct-8b</th>\n",
       "      <td>0.90 ± 0.23</td>\n",
       "      <td>26.41 ± 22.27</td>\n",
       "      <td>0.03 ± 0.16</td>\n",
       "      <td>3.41 ± 1.97</td>\n",
       "      <td>308.33 ± 93.23</td>\n",
       "      <td>6.64 ± 1.99</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-llama/Meta-Llama-3.1-8B-Instruct</th>\n",
       "      <td>0.95 ± 0.15</td>\n",
       "      <td>30.00 ± 20.16</td>\n",
       "      <td>0.05 ± 0.22</td>\n",
       "      <td>3.97 ± 2.27</td>\n",
       "      <td>296.13 ± 113.57</td>\n",
       "      <td>3.82 ± 1.43</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-llama/Meta-Llama-3.1-70B-Instruct</th>\n",
       "      <td>0.92 ± 0.22</td>\n",
       "      <td>35.00 ± 24.73</td>\n",
       "      <td>0.05 ± 0.22</td>\n",
       "      <td>4.85 ± 2.63</td>\n",
       "      <td>261.89 ± 109.02</td>\n",
       "      <td>9.42 ± 4.01</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mfs-rag-baseline</th>\n",
       "      <td>0.90 ± 0.18</td>\n",
       "      <td>42.05 ± 26.91</td>\n",
       "      <td>0.21 ± 0.40</td>\n",
       "      <td>5.36 ± 2.59</td>\n",
       "      <td>119.41 ± 66.23</td>\n",
       "      <td>4.31 ± 1.80</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       answer_relevancy judge_completude  \\\n",
       "AgentPublic/llama3-instruct-8b              0.90 ± 0.23    26.41 ± 22.27   \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct       0.95 ± 0.15    30.00 ± 20.16   \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct      0.92 ± 0.22    35.00 ± 24.73   \n",
       "mfs-rag-baseline                            0.90 ± 0.18    42.05 ± 26.91   \n",
       "\n",
       "                                       judge_exactness judge_notator  \\\n",
       "AgentPublic/llama3-instruct-8b             0.03 ± 0.16   3.41 ± 1.97   \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct      0.05 ± 0.22   3.97 ± 2.27   \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct     0.05 ± 0.22   4.85 ± 2.63   \n",
       "mfs-rag-baseline                           0.21 ± 0.40   5.36 ± 2.59   \n",
       "\n",
       "                                          output_length generation_time  \\\n",
       "AgentPublic/llama3-instruct-8b           308.33 ± 93.23     6.64 ± 1.99   \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct   296.13 ± 113.57     3.82 ± 1.43   \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct  261.89 ± 109.02     9.42 ± 4.01   \n",
       "mfs-rag-baseline                         119.41 ± 66.23     4.31 ± 1.80   \n",
       "\n",
       "                                           toxicity         bias  \n",
       "AgentPublic/llama3-instruct-8b          0.00 ± 0.00  0.00 ± 0.00  \n",
       "meta-llama/Meta-Llama-3.1-8B-Instruct   0.00 ± 0.00  0.00 ± 0.00  \n",
       "meta-llama/Meta-Llama-3.1-70B-Instruct  0.00 ± 0.00  0.00 ± 0.00  \n",
       "mfs-rag-baseline                        0.00 ± 0.00  0.00 ± 0.00  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read results\n",
    "# --\n",
    "df_all = None # multi-dimensional DataFrame\n",
    "arr_all = [] # keep references of source array per metric metrics \n",
    "\n",
    "# Fetch results and compute macro metrics (mean, std etc).\n",
    "# --\n",
    "response = requests.get(f'{API_URL}/v1/experiment_set/{expset_id}')\n",
    "expset = response.json()\n",
    "rows = []\n",
    "for i, exp in enumerate(expset[\"experiments\"]):\n",
    "    # Get an experiment result\n",
    "    exp_id = exp[\"id\"]\n",
    "    response = requests.get(f'{API_URL}/v1/experiment/{exp_id}?with_results=true')\n",
    "    experiment = response.json()\n",
    "    # experiment[\"name\"] # Name of the experiment\n",
    "    if experiment[\"experiment_status\"] != \"finished\":\n",
    "        print(f\"Warning: experiment {exp_id} is not finished yet...\")\n",
    "    results = experiment[\"results\"]\n",
    "    model = experiment[\"model\"][\"name\"] + \"_limit\" + experiment[\"model\"][\"extra_params\"][\"rag\"][\"limit\"]\n",
    "    \n",
    "    #Add an observation row from the observation_table (mean, std etc)\n",
    "    row = {\"model\": model}\n",
    "    rows.append(row)\n",
    "    metric_arrs = {}\n",
    "    arr_all.append(metric_arrs)\n",
    "    for metric_results in results: \n",
    "        metric = metric_results[\"metric_name\"]\n",
    "        arr = np.array([x[\"score\"] for x in metric_results[\"observation_table\"] if pd.notna(x[\"score\"])])\n",
    "        row[(metric, 'mean')] = np.mean(arr)\n",
    "        row[(metric, 'std')] = np.std(arr)\n",
    "        row[(metric, 'median')] = np.median(arr)\n",
    "        row[(metric, 'mean_std')] = f\"{arr.mean():.2f} ± {arr.std():.2f}\"  # Formatting as 'mean±std'\n",
    "        row[(metric, 'support')] = len(arr)\n",
    "        metric_arrs[metric] = arr\n",
    "    \n",
    "df_all = pd.DataFrame(rows)\n",
    "df_all.set_index('model', inplace=True)\n",
    "df_all.columns = pd.MultiIndex.from_tuples(df_all.columns)\n",
    "final_df = df_all.xs('mean', axis=1, level=1)\n",
    "#final_df[\"support\"] = supports # for debugging\n",
    "\n",
    "# Group and average the result of the experiments by models\n",
    "# --\n",
    "def format_metrics(row):\n",
    "   metrics = {}\n",
    "   for metric in final_df.columns.levels[0]:\n",
    "       mean_value = row[(metric, 'mean')]\n",
    "       std_value = row[(metric, 'std')]\n",
    "       metrics[metric] = f\"{mean_value:.2f} ± {std_value:.2f}\"\n",
    "   return pd.Series(metrics)\n",
    "final_df = final_df.groupby(level=0).agg(['mean', 'std'])  # groupby \"model\"\n",
    "final_df.index.name = None\n",
    "final_df = final_df.apply(format_metrics, axis=1) # final formating\n",
    "final_df = final_df.reindex([m[\"name\"] for m in grid_params[\"model\"]]) # reorder rows\n",
    "final_df = final_df[metrics] # reorder columns\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
